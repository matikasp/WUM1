{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cae25f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, silhouette_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60b274ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "       id      base   bonus  overtime_pay    other  sector  section_07  sex  \\\n",
      "0  192064  26651.53     0.0          0.00     0.00       1           3    2   \n",
      "1   25495  40168.50  1500.0          0.00  3414.32       1           2    1   \n",
      "2  142164  20134.80     0.0          0.00  1700.41       1           2    2   \n",
      "3  198034  16475.00     0.0          0.00  1305.00       1           2    2   \n",
      "4  144990  34797.60     0.0       1893.35  3118.73       1           2    2   \n",
      "\n",
      "   education  contract  age  duration_total  duration_entity  \\\n",
      "0          4         1   49           33.03             7.06   \n",
      "1          1         1   36           10.07             6.01   \n",
      "2          4         1   52           28.08            19.05   \n",
      "3          5         1   55           35.07            11.01   \n",
      "4          2         1   50           27.00            19.01   \n",
      "\n",
      "   duration_nominal  duration_overtime  \n",
      "0           1524.15                0.0  \n",
      "1           1562.40                0.0  \n",
      "2           1816.00                0.0  \n",
      "3           1816.00                0.0  \n",
      "4            722.80               63.0  \n"
     ]
    }
   ],
   "source": [
    "# loading the dataset\n",
    "\n",
    "\n",
    "# Load the dataset with proper delimiter and encoding\n",
    "file_path = 'earnings.csv'\n",
    "df = pd.read_csv(file_path, delimiter=';', encoding='utf-8')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1301c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive statistics for quantitative variables:\n",
      "                base          bonus   overtime_pay         other  \\\n",
      "count   11000.000000   11000.000000   11000.000000  11000.000000   \n",
      "mean    33376.738065    2128.486176    1679.273923   2477.625815   \n",
      "std     19276.551638    4966.444726    5407.964344   3715.419120   \n",
      "min        10.000000       0.000000       0.000000      0.000000   \n",
      "25%     20995.115000       0.000000       0.000000      0.000000   \n",
      "50%     31341.245000     620.000000       0.000000   2141.820000   \n",
      "75%     41348.290000    2940.717500    1139.330000   3497.430000   \n",
      "max    241624.390000  258061.000000  228110.340000  88555.760000   \n",
      "\n",
      "                age  duration_total  duration_entity  duration_nominal  \\\n",
      "count  11000.000000     11000.00000     11000.000000      11000.000000   \n",
      "mean      42.470182        18.56708        11.401382       1301.307435   \n",
      "std       10.012140        10.99695         9.322181        540.189234   \n",
      "min       19.000000         0.01000         0.010000         12.800000   \n",
      "25%       34.000000         9.03000         3.070000        766.800000   \n",
      "50%       43.000000        19.03500         9.100000       1591.900000   \n",
      "75%       51.000000        27.11000        18.040000       1768.000000   \n",
      "max       77.000000        57.02000        46.010000       2024.000000   \n",
      "\n",
      "       duration_overtime  \n",
      "count       11000.000000  \n",
      "mean           47.702325  \n",
      "std           111.963867  \n",
      "min             0.000000  \n",
      "25%             0.000000  \n",
      "50%             0.000000  \n",
      "75%            40.000000  \n",
      "max          1812.580000  \n",
      "\n",
      "Frequency table for sector:\n",
      "sector\n",
      "1    10548\n",
      "2      452\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Frequency table for section_07:\n",
      "section_07\n",
      "2    5867\n",
      "3    2732\n",
      "1    2401\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Frequency table for sex:\n",
      "sex\n",
      "2    8289\n",
      "1    2711\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Frequency table for education:\n",
      "education\n",
      "2    6633\n",
      "4    1983\n",
      "5     906\n",
      "3     680\n",
      "1     430\n",
      "6     368\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Frequency table for contract:\n",
      "contract\n",
      "1    9306\n",
      "2    1694\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate descriptive statistics for quantitative variables\n",
    "quantitative_cols = ['base', 'bonus', 'overtime_pay', 'other', 'age', 'duration_total', 'duration_entity', 'duration_nominal', 'duration_overtime']\n",
    "print(\"Descriptive statistics for quantitative variables:\")\n",
    "print(df[quantitative_cols].describe())\n",
    "\n",
    "# Generate frequency tables for qualitative variables\n",
    "qualitative_cols = ['sector', 'section_07', 'sex', 'education', 'contract']\n",
    "for col in qualitative_cols:\n",
    "    print(f\"\\nFrequency table for {col}:\")\n",
    "    print(df[col].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369cbe1c",
   "metadata": {},
   "source": [
    "### TASK 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca855122",
   "metadata": {},
   "source": [
    "## Approach to Variable Selection and Transformation for Clustering Analysis\n",
    "\n",
    "The goal of this analysis is to identify distinct groups of employees based on their earnings and other characteristics using Agglomerative Clustering. This requires careful selection and preparation of variables.\n",
    "\n",
    "### 1. Variable Selection\n",
    "\n",
    "The dataset contains various pieces of information about employees. For clustering, we selected a combination of quantitative (numerical) and qualitative (categorical) variables.\n",
    "\n",
    "**Quantitative Variables Chosen:**\n",
    "\n",
    "*   `base`: Base salary.\n",
    "*   `bonus`: Bonus amount.\n",
    "*   `overtime_pay`: Pay for overtime work.\n",
    "*   `other`: Other forms of remuneration.\n",
    "*   `age`: Age of the employee.\n",
    "*   `duration_total`: Total duration of employment in years.\n",
    "*   `duration_entity`: Duration of employment in the current entity in years.\n",
    "*   `duration_nominal`: Nominal hours worked.\n",
    "*   `duration_overtime`: Overtime hours worked.\n",
    "\n",
    "**Justification for Quantitative Variables:**\n",
    "These variables represent the primary financial components of an employee's earnings and key demographic/employment history aspects (age, tenure, work hours). They are inherently numerical and provide a strong basis for differentiating employee groups based on their compensation structure and work patterns. The `id` column was excluded as it serves only as an identifier and holds no analytical value for clustering.\n",
    "\n",
    "**Qualitative Variables Chosen:**\n",
    "\n",
    "*   `sector`: Sector of employment (e.g., public, private - numerically coded in the data).\n",
    "*   `section_07`: Section based on NACE classification (numerically coded).\n",
    "*   `sex`: Sex of the employee (numerically coded, e.g., 1 for Male, 2 for Female).\n",
    "*   `education`: Level of education (numerically coded).\n",
    "*   `contract`: Type of contract (numerically coded).\n",
    "\n",
    "**Justification for Qualitative Variables:**\n",
    "These variables offer important context about the employees and their work environment. Although they are already numerically coded in the provided dataset, they represent distinct categories. Including them can help in forming more meaningful and interpretable clusters. For instance, clusters might emerge that are predominantly from a specific sector, or have a particular education level or contract type.\n",
    "\n",
    "### 2. Variable Transformations\n",
    "\n",
    "To prepare the data for the Agglomerative Clustering algorithm, two main types of transformations were applied:\n",
    "\n",
    "**a. Encoding Qualitative Variables:**\n",
    "\n",
    "*   **Method Used:** `LabelEncoder` from the `scikit-learn` library.\n",
    "*   **Transformation Details:** `LabelEncoder` was applied to each of the selected qualitative variables (`sector`, `section_07`, `sex`, `education`, `contract`). This encoder assigns a unique integer to each unique category within a feature. For example, if 'sex' has categories 'Male' (coded as 1 in the original data) and 'Female' (coded as 2), `LabelEncoder` might transform these into 0 and 1 respectively (or a similar 0-indexed integer mapping).\n",
    "*   **Justification:**\n",
    "    *   **Algorithm Requirement:** Agglomerative Clustering operates on numerical data.\n",
    "    *   **Categorical Nature:** The qualitative variables, despite being numerically coded in the source file, represent categories. `LabelEncoder` ensures these are treated as distinct categorical inputs and are converted to a standard format (typically 0 to n-1 classes) suitable for many machine learning algorithms.\n",
    "    *   **Simplicity:** While One-Hot Encoding is an alternative that avoids imposing an ordinal relationship (which `LabelEncoder` might implicitly suggest if the numbers were arbitrary), `LabelEncoder` was chosen for its simplicity in this initial approach. Given that the original data already used numerical codes for these categories, `LabelEncoder` primarily serves to standardize these into a 0-indexed format if they weren't already.\n",
    "\n",
    "**b. Scaling Quantitative Variables:**\n",
    "\n",
    "*   **Method Used:** `StandardScaler` from the `scikit-learn` library.\n",
    "*   **Transformation Details:** `StandardScaler` was applied to all selected quantitative variables. This method standardizes features by subtracting the mean and then dividing by the standard deviation. Each transformed feature will have a mean of 0 and a standard deviation of 1. The formula for a sample `x` is: `z = (x - u) / s` (where `u` is the mean and `s` is the standard deviation).\n",
    "*   **Justification:**\n",
    "    *   **Distance-Based Algorithm:** Agglomerative Clustering relies on distance measures (typically Euclidean distance) to determine the similarity between data points.\n",
    "    *   **Preventing Dominance:** Variables with larger absolute values or wider ranges (e.g., `base` salary) can disproportionately influence these distance calculations compared to variables with smaller values (e.g., `age`), potentially skewing the clustering results.\n",
    "    *   **Equal Contribution:** `StandardScaler` transforms the data so that all quantitative variables have a similar scale (mean of 0, standard deviation of 1). This ensures that each variable contributes more equally to the clustering process, leading to more balanced and meaningful cluster formations.\n",
    "\n",
    "By performing these selection and transformation steps, the dataset is appropriately prepared for the Agglomerative Clustering algorithm, enabling it to effectively identify underlying patterns and group similar employees based on a comprehensive set of their financial and descriptive characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "044f4b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation for Task 3 classification models complete.\n",
      "X_train_processed shape: (7700, 18)\n",
      "X_test_processed shape: (3300, 18)\n",
      "y_train distribution:\n",
      "has_higher_education\n",
      "1    0.642078\n",
      "0    0.357922\n",
      "Name: proportion, dtype: float64\n",
      "y_test distribution:\n",
      "has_higher_education\n",
      "1    0.642121\n",
      "0    0.357879\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- Task 3: Data Preparation for Classification Models ---\n",
    "# Ensure df is loaded from 'earnings.csv' (from Task 1)\n",
    "# Required imports: pandas, numpy, ColumnTransformer, StandardScaler, OneHotEncoder, train_test_split\n",
    "\n",
    "df_task3 = df.copy() # Work on a copy\n",
    "\n",
    "# 1. Create the target variable 'has_higher_education'\n",
    "df_task3['has_higher_education'] = (df_task3['education'] <= 2).astype(int)\n",
    "\n",
    "# 2. Define features (X) and target (y)\n",
    "# Numerical features (excluding 'education' as it forms the target)\n",
    "numerical_features = ['age', 'base', 'bonus', 'overtime_pay', 'other', \n",
    "                    'duration_total', 'duration_entity', 'duration_nominal', 'duration_overtime']\n",
    "# Categorical features (excluding 'education')\n",
    "categorical_features = ['sector', 'section_07', 'sex', 'contract']\n",
    "\n",
    "# Ensure all specified features exist in the DataFrame\n",
    "all_expected_features = numerical_features + categorical_features\n",
    "existing_cols_in_df = df_task3.columns.tolist()\n",
    "\n",
    "# Filter features to only those present in the DataFrame to avoid errors\n",
    "numerical_features = [col for col in numerical_features if col in existing_cols_in_df]\n",
    "categorical_features = [col for col in categorical_features if col in existing_cols_in_df]\n",
    "\n",
    "if not (numerical_features + categorical_features):\n",
    "    raise ValueError(\"No features selected or available in the DataFrame. Check feature lists and df columns.\")\n",
    "\n",
    "X = df_task3[numerical_features + categorical_features]\n",
    "y = df_task3['has_higher_education']\n",
    "\n",
    "# 3. Create a preprocessor object using ColumnTransformer\n",
    "transformers = []\n",
    "if numerical_features:\n",
    "    transformers.append(('num', StandardScaler(), numerical_features))\n",
    "if categorical_features:\n",
    "    # handle_unknown='ignore' will prevent errors if new categories appear in test data\n",
    "    transformers.append(('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features))\n",
    "\n",
    "if not transformers:\n",
    "    raise ValueError(\"No transformers defined. Ensure numerical_features or categorical_features lists are populated.\")\n",
    "    \n",
    "preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "\n",
    "# 4. Split data into training and testing sets\n",
    "# stratify=y is good for classification tasks to maintain class proportion in splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# 5. Apply preprocessing\n",
    "# Fit the preprocessor on the training data and transform both training and test data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Data preparation for Task 3 classification models complete.\")\n",
    "print(f\"X_train_processed shape: {X_train_processed.shape}\")\n",
    "print(f\"X_test_processed shape: {X_test_processed.shape}\")\n",
    "print(\"y_train distribution:\")\n",
    "print(f\"{y_train.value_counts(normalize=True)}\")\n",
    "print(\"y_test distribution:\")\n",
    "print(f\"{y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c03ace74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Logistic Regression Results ---\n",
      "Logistic Regression Accuracy: 0.8582\n",
      "\n",
      "Logistic Regression Confusion Matrix:\n",
      "[[ 944  237]\n",
      " [ 231 1888]]\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80      1181\n",
      "           1       0.89      0.89      0.89      2119\n",
      "\n",
      "    accuracy                           0.86      3300\n",
      "   macro avg       0.85      0.85      0.85      3300\n",
      "weighted avg       0.86      0.86      0.86      3300\n",
      "\n",
      "\n",
      "Unique values in y_test: [0 1]\n",
      "Unique values in y_pred_log_reg: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# --- Logistic Regression Model ---\n",
    "# X_train_processed, y_train, X_test_processed, y_test should be available from Task 3 data preparation\n",
    "\n",
    "print(\"\\n--- Logistic Regression Results ---\")\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "log_reg_model = LogisticRegression(random_state=42, solver='liblinear', max_iter=1000)\n",
    "log_reg_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_log_reg = log_reg_model.predict(X_test_processed)\n",
    "\n",
    "# Evaluate the model\n",
    "log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "log_reg_conf_matrix = confusion_matrix(y_test, y_pred_log_reg)\n",
    "log_reg_class_report = classification_report(y_test, y_pred_log_reg, zero_division=0)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {log_reg_accuracy:.4f}\")\n",
    "print(\"\\nLogistic Regression Confusion Matrix:\")\n",
    "print(log_reg_conf_matrix)\n",
    "print(\"\\nLogistic Regression Classification Report:\")\n",
    "print(log_reg_class_report)\n",
    "\n",
    "# Print unique values in y_test and y_pred_log_reg to understand the report better\n",
    "print(f\"\\nUnique values in y_test: {np.unique(y_test)}\")\n",
    "print(f\"Unique values in y_pred_log_reg: {np.unique(y_pred_log_reg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e74ddb",
   "metadata": {},
   "source": [
    "### Assessing Logistic Regression Model Quality with Cross-Validation\n",
    "\n",
    "To get a more reliable estimate of the Logistic Regression model's performance on unseen data, we use k-fold cross-validation. This technique helps to mitigate issues like overfitting to a specific train-test split and provides a more robust measure of generalization.\n",
    "\n",
    "**Methodology:**\n",
    "\n",
    "1.  **Pipeline:** A pipeline was constructed combining the same preprocessing steps (Standard Scaling for numerical features, One-Hot Encoding for categorical features) and the Logistic Regression model. This ensures that preprocessing is applied correctly and independently within each fold of the cross-validation, preventing data leakage.\n",
    "2.  **Data:** The cross-validation was performed on the original training set (`X_train`, `y_train`).\n",
    "3.  **K-Fold Strategy:** We used Stratified K-Fold with 5 splits (`StratifiedKFold(n_splits=5, shuffle=True, random_state=42)`). Stratification ensures that each fold maintains approximately the same percentage of samples of each target class as the complete set, which is important for classification tasks, especially with imbalanced datasets.\n",
    "4.  **Metrics:** The model was evaluated using Accuracy, Weighted Precision, Weighted Recall, and Weighted F1-Score.\n",
    "\n",
    "**Interpretation of Results:**\n",
    "\n",
    "The output from the cross-validation provides the mean and standard deviation for each metric across the 5 folds:\n",
    "\n",
    "*   **Mean:** The average performance for that metric. This is our primary estimate of how the model is expected to perform on unseen data.\n",
    "*   **Standard Deviation (Std):** This indicates the variability of the metric. A smaller standard deviation suggests consistent performance across different data subsets.\n",
    "\n",
    "These cross-validated scores are generally a better indicator of the model's true predictive power on new data than scores from a single train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "546163d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Linear Discriminant Analysis (LDA) Results ---\n",
      "LDA Accuracy: 0.8482\n",
      "\n",
      "LDA Confusion Matrix:\n",
      "[[ 947  234]\n",
      " [ 267 1852]]\n",
      "\n",
      "LDA Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79      1181\n",
      "           1       0.89      0.87      0.88      2119\n",
      "\n",
      "    accuracy                           0.85      3300\n",
      "   macro avg       0.83      0.84      0.84      3300\n",
      "weighted avg       0.85      0.85      0.85      3300\n",
      "\n",
      "\n",
      "Unique values in y_test: [0 1]\n",
      "Unique values in y_pred_lda: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# --- Linear Discriminant Analysis (LDA) Model ---\n",
    "# X_train_processed, y_train, X_test_processed, y_test are from Task 3 data preparation\n",
    "\n",
    "print(\"\\n--- Linear Discriminant Analysis (LDA) Results ---\")\n",
    "\n",
    "# Initialize and train the LDA model\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_lda = lda_model.predict(X_test_processed)\n",
    "\n",
    "# Evaluate the model\n",
    "lda_accuracy = accuracy_score(y_test, y_pred_lda)\n",
    "lda_conf_matrix = confusion_matrix(y_test, y_pred_lda)\n",
    "lda_class_report = classification_report(y_test, y_pred_lda, zero_division=0)\n",
    "\n",
    "print(f\"LDA Accuracy: {lda_accuracy:.4f}\")\n",
    "print(\"\\nLDA Confusion Matrix:\")\n",
    "print(lda_conf_matrix)\n",
    "print(\"\\nLDA Classification Report:\")\n",
    "print(lda_class_report)\n",
    "\n",
    "print(f\"\\nUnique values in y_test: {np.unique(y_test)}\")\n",
    "print(f\"Unique values in y_pred_lda: {np.unique(y_pred_lda)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad7927e",
   "metadata": {},
   "source": [
    "### Assessing LDA Model Quality with Cross-Validation\\n\n",
    "\\n\n",
    "To get a more reliable estimate of the Linear Discriminant Analysis (LDA) model's performance on unseen data, we use k-fold cross-validation. This technique helps to mitigate issues like overfitting to a specific train-test split and provides a more robust measure of generalization.\\n\n",
    "\\n\n",
    "**Methodology:**\\n\n",
    "\\n\n",
    "1.  **Pipeline:** A pipeline was constructed combining the same preprocessing steps (Standard Scaling for numerical features, One-Hot Encoding for categorical features) and the LDA model. This ensures that preprocessing is applied correctly and independently within each fold of the cross-validation, preventing data leakage.\\n\n",
    "2.  **Data:** The cross-validation was performed on the original training set (`X_train`, `y_train`).\\n\n",
    "3.  **K-Fold Strategy:** We used Stratified K-Fold with 5 splits (`StratifiedKFold(n_splits=5, shuffle=True, random_state=42)`). Stratification ensures that each fold maintains approximately the same percentage of samples of each target class as the complete set, which is important for classification tasks, especially with imbalanced datasets.\\n\n",
    "4.  **Metrics:** The model was evaluated using Accuracy, Weighted Precision, Weighted Recall, and Weighted F1-Score.\\n\n",
    "\\n\n",
    "**Interpretation of Results:**\\n\n",
    "\\n\n",
    "The output from the cross-validation provides the mean and standard deviation for each metric across the 5 folds:\\n\n",
    "\\n\n",
    "*   **Mean:** The average performance for that metric. This is our primary estimate of how the LDA model is expected to perform on unseen data.\\n\n",
    "*   **Standard Deviation (Std):** This indicates the variability of the metric. A smaller standard deviation suggests consistent performance across different data subsets.\\n\n",
    "\\n\n",
    "These cross-validated scores are generally a better indicator of the model's true predictive power on new data than scores from a single train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7e576",
   "metadata": {},
   "source": [
    "### Assessing QDA Model Quality with Cross-Validation\\n\n",
    "\\n\n",
    "To obtain a more robust evaluation of the Quadratic Discriminant Analysis (QDA) model's performance and its ability to generalize to unseen data, we employ k-fold cross-validation.\\n\n",
    "\\n\n",
    "**Methodology:**\\n\n",
    "\\n\n",
    "1.  **Pipeline:** A pipeline was created, integrating the same preprocessing steps used for other models (Standard Scaling for numerical features, One-Hot Encoding for categorical features) with the QDA classifier. This ensures that preprocessing is applied consistently and independently within each fold, preventing data leakage from the validation set into the training process of that fold.\\n\n",
    "2.  **Data:** The cross-validation was performed on the original training dataset (`X_train`, `y_train`).\\n\n",
    "3.  **K-Fold Strategy:** We utilized Stratified K-Fold with 5 splits (`StratifiedKFold(n_splits=5, shuffle=True, random_state=42)`). Stratification ensures that each fold reflects the class distribution of the original dataset, which is crucial for reliable classification model evaluation, particularly with imbalanced classes.\\n\n",
    "4.  **Metrics:** The QDA model was evaluated using key classification metrics: Accuracy, Weighted Precision, Weighted Recall, and Weighted F1-Score.\\n\n",
    "\\n\n",
    "**Interpretation of Results:**\\n\n",
    "\\n\n",
    "The cross-validation process yields the mean and standard deviation for each evaluation metric across the 5 folds:\\n\n",
    "\\n\n",
    "*   **Mean:** This represents the average performance of the QDA model for a given metric. It serves as our primary estimate of how the model is likely to perform on new, unseen data.\\n\n",
    "*   **Standard Deviation (Std):** This measures the variability or spread of the metric scores across the different folds. A lower standard deviation indicates more consistent performance, suggesting that the model is not overly sensitive to the specific subsets of data used for training and validation within the folds.\\n\n",
    "\\n\n",
    "These cross-validated scores provide a more dependable assessment of the QDA model's predictive capabilities compared to scores from a single train-test split, as they account for variability in data sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80f9407f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- K-Nearest Neighbors (KNN) Results ---\n",
      "KNN Accuracy (k=5): 0.8512\n",
      "\n",
      "KNN Confusion Matrix (k=5):\n",
      "[[ 953  228]\n",
      " [ 263 1856]]\n",
      "\n",
      "KNN Classification Report (k=5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.80      1181\n",
      "           1       0.89      0.88      0.88      2119\n",
      "\n",
      "    accuracy                           0.85      3300\n",
      "   macro avg       0.84      0.84      0.84      3300\n",
      "weighted avg       0.85      0.85      0.85      3300\n",
      "\n",
      "\n",
      "Unique values in y_test: [0 1]\n",
      "Unique values in y_pred_knn: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# --- K-Nearest Neighbors (KNN) Model ---\n",
    "# X_train_processed, y_train, X_test_processed, y_test are from Task 3 data preparation\n",
    "\n",
    "print(\"\\n--- K-Nearest Neighbors (KNN) Results ---\")\n",
    "\n",
    "# Initialize and train the KNN model (e.g., with k=5 neighbors)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_knn = knn_model.predict(X_test_processed)\n",
    "\n",
    "# Evaluate the model\n",
    "knn_accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "knn_conf_matrix = confusion_matrix(y_test, y_pred_knn)\n",
    "knn_class_report = classification_report(y_test, y_pred_knn, zero_division=0)\n",
    "\n",
    "print(f\"KNN Accuracy (k=5): {knn_accuracy:.4f}\")\n",
    "print(\"\\nKNN Confusion Matrix (k=5):\")\n",
    "print(knn_conf_matrix)\n",
    "print(\"\\nKNN Classification Report (k=5):\")\n",
    "print(knn_class_report)\n",
    "\n",
    "print(f\"\\nUnique values in y_test: {np.unique(y_test)}\")\n",
    "print(f\"Unique values in y_pred_knn: {np.unique(y_pred_knn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7e576",
   "metadata": {},
   "source": [
    "### Assessing QDA Model Quality with Cross-Validation\\n\n",
    "\\n\n",
    "To obtain a more robust evaluation of the Quadratic Discriminant Analysis (QDA) model's performance and its ability to generalize to unseen data, we employ k-fold cross-validation.\\n\n",
    "\\n\n",
    "**Methodology:**\\n\n",
    "\\n\n",
    "1.  **Pipeline:** A pipeline was created, integrating the same preprocessing steps used for other models (Standard Scaling for numerical features, One-Hot Encoding for categorical features) with the QDA classifier. This ensures that preprocessing is applied consistently and independently within each fold, preventing data leakage from the validation set into the training process of that fold.\\n\n",
    "2.  **Data:** The cross-validation was performed on the original training dataset (`X_train`, `y_train`).\\n\n",
    "3.  **K-Fold Strategy:** We utilized Stratified K-Fold with 5 splits (`StratifiedKFold(n_splits=5, shuffle=True, random_state=42)`). Stratification ensures that each fold reflects the class distribution of the original dataset, which is crucial for reliable classification model evaluation, particularly with imbalanced classes.\\n\n",
    "4.  **Metrics:** The QDA model was evaluated using key classification metrics: Accuracy, Weighted Precision, Weighted Recall, and Weighted F1-Score.\\n\n",
    "\\n\n",
    "**Interpretation of Results:**\\n\n",
    "\\n\n",
    "The cross-validation process yields the mean and standard deviation for each evaluation metric across the 5 folds:\\n\n",
    "\\n\n",
    "*   **Mean:** This represents the average performance of the QDA model for a given metric. It serves as our primary estimate of how the model is likely to perform on new, unseen data.\\n\n",
    "*   **Standard Deviation (Std):** This measures the variability or spread of the metric scores across the different folds. A lower standard deviation indicates more consistent performance, suggesting that the model is not overly sensitive to the specific subsets of data used for training and validation within the folds.\\n\n",
    "\\n\n",
    "These cross-validated scores provide a more dependable assessment of the QDA model's predictive capabilities compared to scores from a single train-test split, as they account for variability in data sampling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
